### TCP
IP首部，TCP首部，UDP首部
```
TCP首部:  
        16位源端口  | 16位目的端口
            32位序列号
            32位确认号
        4位头部长度|6位保留字段|SYN|ACK|RST| 16位窗口
        16位校验和|16位紧急指针
IP首部:
IP首部最小是20个字节
IP数据报大于1500字节，发送方IP层就需要将数据包分成若干片
UDP首部:
    源端口|目的端口|长度|校验和

```
MTU长度问题
```
TCP整个包的最大长度是由最大传输大小决定。
UDP整个包的du最大长度为zhi65535
MTU:v1518，头信息有14字节，尾部校验和FCS占了4字节，所以真正留给上层协议传输数据的大小就是：1518 - 14 - 4 = 1500
解释:
    在链路层，由以太网的物理特性决定了数据帧的长度为(46＋18)－(1500＋18)，其中的18是数据帧的头和尾，也就是说数据帧的内容最大为1500(不包括帧头和帧尾)，
    即MTU(Maximum Transmission Unit)为1500； 　
    在网络层，因为IP包的首部要占用20字节，所以这的MTU为1500－20＝1480；　
    在传输层，对于UDP包的首部要占用8字节，所以这的MTU为1480－8＝1472； 　　
    所以，在应用层，你的Data最大长度为1472。当我们的UDP包中的数据多于MTU(1472)时，发送方的IP层需要分片fragmentation进行传输，
    而在接收方IP层则需要进行数据报重组，由于UDP是不可靠的传输协议，如果分片丢失导致重组失败，将导致UDP数据包被丢弃。　　
    从上面的分析来看，在普通的局域网环境下，UDP的数据最大为1472字节最好(避免分片重组)。 　　
    但在网络编程中，Internet中的路由器可能有设置成不同的值(小于默认值)，Internet上的标准MTU值为576，
    所以Internet的UDP编程时数据长度最好在576－20－8＝548字节以内。
UDP 包的大小就应该是 1500 - IP头(20) - UDP头(8) = 1472(Bytes)
TCP 包的大小就应该是 1500 - IP头(20) - TCP头(20) = 1460 (Bytes)
    数据报的最大长度根据操作环境的不同而各异。从理论上说，包含报头在内的数据报的最大长度为65535字节(64K)。
    不过鉴于Internet(非局域网)上的标准MTU值为576字节，所以建议在进行Internet的UDP编程时，最好将UDP的数据长度控制在548字节 (576-8-20)以内。
    因为在链路层的MAC子层中会进行数据补齐，不足18个字节的用0补齐。
    但当服务器在公网，客户端在内网，发生小于18个字节的数据，就会出现接收端收不到数据的情况。
       以太网EthernetII规定，以太网帧数据域部分最小为46字节，也就是以太网帧最小是6＋6＋2＋46＋4＝64。除去4个字节的FCS，因此，抓包时就是60字节。
       当数据字段的长度小于46字节时，MAC子层就会在数据字段的后面填充以满足数据帧长不小于64字节。
       由于填充数据是由MAC子层负责，也就是设备驱动程序。不同的抓包程序和设备驱动程序所处的优先层次可能不同，抓包程序的优先级可能比设备驱动程序更高，
       也就是说，我们的抓包程序可能在设备驱动程序还没有填充不到64字节的帧的时候，抓包程序已经捕获了数据。
       因此不同的抓包工具抓到的数据帧的大小可能不同。下列是本人分别用wireshark和sniffer抓包的结果，
       对于TCP 的ACK确认帧的大小一个是54字节，一个是60字节，wireshark抓取时没有填充数据段，sniffer抓取时有填充数据段。
```
(2) TCP和UDP区别
```
TCP基于有连接，UDP基于无连接。
    有连接就是TCP在传输前先发送连接请求和应答包，确定双方能够正常传输后，才开始进行数据传输。
    无连接就是UDP在发送数据之前，并不考虑对方能否接受到，甚至目的地址可能都是无效；
TCP能保证可靠传输，UDP不能保证可靠传输TCP。
    所谓可靠就是TCP能保证把数据一定送到目的地址。
    为了实现可靠，TCP采用有连接的，超时重传，应答机制等。
    而UDP则没有这些，也不能保证数据一定能送到；
TCP结构复杂，消耗资源多，建立过程较慢较复杂。
    UDP结构简单，消耗资源少，建立过程较快；
TCP基于流模式，UDP是数据报模式。
    TCP把数据看成一连串无结构的字节流，没有边界，一段段传输构成了整个数据块。
    通过发送缓冲区和接受缓冲区来存储数据流。
    而UDP数据报模式，每一个数据报都是一个独立的对象，有着指定的大小。
TCP连接只能是点到点，而UDP可以一对一，一对多或者多对多。
    TCP只能是点到点原因很简单，因为TCP的传输前要先建立连接。
    因此，广播和多播只能采用UDP数据报的方式。
TCP有确认，重传，拥赛控制机制，UDP在没有建立连接或者对方已经退出的情况下任然会继续发送数据，导致通信流量的浪费。
```
(3) TCP和UDP应用场景
```
    TCP适用于可靠性要求高的地方入文件传输
    UDP适用于实时应用 IP电话 视频会议 直播等
```
(4) 如何实现可靠的UDP
```
1、添加seq/ack机制，确保数据发送到对端
2、添加发送和接收缓冲区，主要是用户超时重传。
3、添加超时重传机制。
```
三次握手和四次挥手状态变化；
```
三次握手:
    SYNc   ----> 
    (此时状态为SYN_SENT)
        <------ SYNs ACKc  其中ACKc = SYNc+1
    (此时状态为SYN_RECV)
    ACKc-------->          其中ACKc=SYNs+1
    (此时状态为ESTABLISHED)

四次挥手:
    FIN -------->
    (进入FIN_WAIT_1状态)
    <----------- ACK
    (进入FIN_WAIT_2状态) 
     
    <---------------- FIN
    (进入TIME_WAIT状态,等待2MSL后即可回到CLOSED状态)
    ACK ---------------->
```  
为什么建立连接协议是三次握手，而关闭连接却是四次握手呢？
```
    这是因为服务端的LISTEN状态下的SOCKET当收到SYN的报文的建连请求后,它可以把ACK和SYN放在一个报文里发送,
    但关闭连接时,当收到对方的FIN报文通知时,他仅仅表示对方没有数据发送,未必你所有的数据都发送给对方了，
    所以你未必会马上关闭SOCKET,所以这里报文多数情况下是要分开发送的;
```
(2) 2MSL是什么状态？作用是什么？
```
    虽然双方都同意关闭连接了,而且握手的4个报文也都发送完毕,按理可以直接回到CLOSED状态,
    但是,我们必须假想网络时不可靠的,无法保证你最后发送的ACK报文一定会被对方收到,
    即对方处于LAST_ACK状态下的SOCKET可能因为超时未收到ACK报文而重发FIN报文，
    所以这个TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文
```
TCP如何保证可靠传输的？
```
连接管理 --- 三次握手和四次挥手
数据破坏 --- 通过校验和
丢包     --- 应答与超时重复机制
分片乱序 --- 序列号
滑动窗口 --- 提高发送效率,对发送端和接收端流量进行控制
加快通信速度 --- 快速重发,三次收到重发消息进行重发
流控制  ---- 避免网络流量消费
拥塞控制 --- 慢启动算法 拥塞窗口
```
Nagle算法
```
数据在发送端被缓存,如果缓存到达指定大小就将其发送,或者上一个数据的应答包到达,将缓存取一次性全部发送；
Nagle算法从发送端角度考虑减少了数据包的个数,时延应答从接受端角度考虑减少了数据包的个数
```
TCP窗口
```
滑动窗口协议的基本原理就是在任意时刻，发送方都维持了一个连续的允许发送的帧的序号，称为发送窗口；
同时，接收方也维持了一个连续的允许接收的帧的序号，称为接收窗口
比特滑动窗口协议
 当发送窗口和接收窗口的大小固定为1时，滑动窗口协议退化为停等协议
后退n协议
  发送方在发完一个数据帧后，不停下来等待应答帧，而是连续发送若干个数据帧，即使在连续发送过程中收到了接收方发来的应答帧，也可以继续发送。且发送方在每发送完一个数据帧时都要设置超时定时器。只要在所设置的超时时间内仍收到确认帧，就要重发相应的数据帧
选择重传协议
    当接收方发现某帧出错后，其后继续送来的正确的帧虽然不能立即递交给接收方的高层，但接收方仍可收下来，存放在一个缓冲区中，同时要求发送方重新传送出错的那一帧。一旦收到重新传来的帧后，就可以原已存于缓冲区中的其余帧一并按正确的顺序递交高层
```
TCP客户与服务器模型，用到哪些函数
```
#include <sys/socket.h>
 int socket(int family,int type,int protocol); 　　　
  　　 　返回：非负描述字－－－成功　　　-1－－－失败

#include <sys/socket.h> 　
int bind(int sockfd, const struct sockaddr * server, socklen_t addrlen);
 返回：0－－－成功　　　-1－－－失败　

   #include<sys/socket.h>
int listen(int sockfd, int backlog);
TCP的三次握手是在调用connect函数时完成的，服务器端没有调用函数，但是必须有套接字在某个端口监听，不然会返回客户端RST，终止连接。

#include <sys/socket.h> 　　 　 　 　
int accept(int listenfd, struct sockaddr *client, socklen_t * addrlen); 　

#include <unistd.h> 　　 　 　 　
 int write(int sockfd, char *buf, int len);　
  回：非负－－－成功　　　-1－－－失败

  #include <unistd.h> 　　 　 　 　
 int read(int sockfd, char *buf, intlen); 　
  回：非负－－－成功　　　-1－－－失败

   #include <sys/socket.h>　　 　 　
 int connect(int sockfd, const struct sockaddr * addr, socklen_t addrlen); 　
 返回：0－－－成功　　　-1－－－失败
 如果客户端没有收到SYN的响应包，根据TCP的超时重发机制进行重发。75秒后还没收到，就返回错误。
2. 如果目的主机没有监听目的端口号，就会返回一个RST的分节，客户端收到RST后立刻返回错误。
3. 如果SYN在中间路由遇到目的不可达，客户端收到ICMP报文，客户端保存这个报文信息，并采用第一种情况方案解决，也就是重发
```
UDP客户与服务器模型，用到哪些函数
```
#include <sys/types.h>
#include <sys/socket.h>
sockfd = socket(AF_INET, SOCK_DGRAM, 0)；

#include <sys/types.h>          /* See NOTES */
#include <sys/socket.h>
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

#include <sys/types.h>
#include <sys/socket.h>

ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen);
```
域名解析过程
```
1、在浏览器中输入www.qq.com 域名，操作系统会先检查自己本地的hosts文件是否有这个网址映射关系，如果有，就先调用这个IP地址映射，完成域名解析。 
2、如果hosts里没有这个域名的映射，则查找本地DNS解析器缓存，是否有这个网址映射关系，如果有，直接返回，完成域名解析。 
3、如果hosts与本地DNS解析器缓存都没有相应的网址映射关系，首先会找TCP/ip参数中设置的首选DNS服务器，在此我们叫它本地DNS服务器，此服务器收到查询时，
    如果要查询的域名，包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析，此解析具有权威性。 
4、如果要查询的域名，不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析，此解析不具有权威性。 
5、如果本地DNS服务器本地区域文件与缓存解析都失效，则根据本地DNS服务器的设置（是否设置转发器）进行查询，如果未用转发模式，本地DNS就把请求发至13台根DNS，
    根DNS服务器收到请求后会判断这个域名(.com)是谁来授权管理，并会返回一个负责该顶级域名服务器的一个IP。本地DNS服务器收到IP信息后，
    将会联系负责.com域的这台服务器。这台负责.com域的服务器收到请求后，如果自己无法解析，
    它就会找一个管理.com域的下一级DNS服务器地址(http://qq.com)给本地DNS服务器。当本地DNS服务器收到这个地址后，就会找http://qq.com域服务器，
    重复上面的动作，进行查询，直至找到www.qq.com主机。 
6、如果用的是转发模式，此DNS服务器就会把请求转发至上一级DNS服务器，由上一级服务器进行解析，上一级服务器如果不能解析，
    或找根DNS或把转请求转至上上级，以此循环。不管是本地DNS服务器用是是转发，还是根提示，
    最后都是把结果返回给本地DNS服务器，由此DNS服务器再返回给客户机。
```
ARP的机制
```
ARP协议的主要工作就是建立、查询、更新、删除ARP表项
```
Ping和TraceRoute实现原理
```
(1) Ping是通过发送ICMP报文回显请求实现。
(2) TraceRoute通过发送UDP报文，设置目的端口为一个不可能的值，将IP首部中的TTL分别设置从1到N，每次逐个增加，如果收到端口不可达，说明到达目的主机，如果是因为TTL跳数超过，路由器会发送主机不可达的ICMP报文。
```
###HTTP
http的主要特点:
```
简单快速：当客户端向服务器端发送请求时，只是简单的填写请求路径和请求方法即可，然后就可以通过浏览器或其他方式将该请求发送就行了
灵活：HTTP 协议允许客户端和服务器端传输任意类型任意格式的数据对象
无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接，采用这种方式可以节省传输时间。
        (当今多数服务器支持Keep-Alive功能，使用服务器支持长连接，解决无连接的问题)
无状态：无状态是指协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。
        即客户端发送HTTP请求后，服务器根据请求，会给我们发送数据，发送完后，不会记录信息。(使用 cookie 机制可以保持 session，解决无状态的问题)
```
http1.1的特点
```
a、默认持久连接节省通信量，只要客户端服务端任意一端没有明确提出断开TCP连接，就一直保持连接，可以发送多次HTTP请求
b、管线化，客户端可以同时发出多个HTTP请求，而不用一个个等待响应
c、断点续传ftghh
```
http2.0的特点
```
a、HTTP/2采用二进制格式而非文本格式
b、HTTP/2是完全多路复用的，而非有序并阻塞的——只需一个HTTP连接就可以实现多个请求响应
c、使用报头压缩，HTTP/2降低了开销
d、HTTP/2让服务器可以将响应主动“推送”到客户端缓存中
```
get/post 区别
```
区别一：
    get重点在从服务器上获取资源，post重点在向服务器发送数据；
区别二：
    get传输数据是通过URL请求，以field（字段）= value的形式，置于URL后，并用"?"连接，多个请求数据间用"&"连接，
    如http://127.0.0.1/Test/login.action?name=admin&password=admin，这个过程用户是可见的；
    post传输数据通过Http的post机制，将字段与对应值封存在请求实体中发送给服务器，这个过程对用户是不可见的；
区别三：
    Get传输的数据量小，因为受URL长度限制，但效率较高；
    Post可以传输大量数据，所以上传文件时只能用Post方式；
区别四：
    get是不安全的，因为URL是可见的，可能会泄露私密信息，如密码等；
    post较get安全性较高；
```
返回状态码
```
200：请求被正常处理
204：请求被受理但没有资源可以返回
206：客户端只是请求资源的一部分，服务器只对请求的部分资源执行GET方法，相应报文中通过Content-Range指定范围的资源。
301：永久性重定向
302：临时重定向
303：与302状态码有相似功能，只是它希望客户端在请求一个URI的时候，能通过GET方法重定向到另一个URI上
304：发送附带条件的请求时，条件不满足时返回，与重定向无关
307：临时重定向，与302类似，只是强制要求使用POST方法
400：请求报文语法有误，服务器无法识别
401：请求需要认证
403：请求的对应资源禁止被访问
404：服务器无法找到对应资源
500：服务器内部错误
503：服务器正忙
HTTP协议的响应报文由状态行、响应头部和响应包体组成，其响应状态码总体描述如下：
1xx：指示信息--表示请求已接收，继续处理。
2xx：成功--表示请求已被成功接收、理解、接受。
3xx：重定向--要完成请求必须进行更进一步的操作。
4xx：客户端错误--请求有语法错误或请求无法实现。
5xx：服务器端错误--服务器未能实现合法的请求。
常见状态代码、状态描述的详细说明如下。
200 OK：客户端请求成功。
206 partial content服务器已经正确处理部分GET请求，实现断点续传或同时分片下载，
    该请求必须包含Range请求头来指示客户端期望得到的范围
300 multiple choices（可选重定向）:被请求的资源有一系列可供选择的反馈信息，由浏览器/用户自行选择其中一个。
301  moved permanently（永久重定向）：该资源已被永久移动到新位置，将来任何对该资源的访问都要使用本响应返回的若干个URI之一。
302 move temporarily(临时重定向)：请求的资源现在临时从不同的URI中获得，
304：not modified :如果客户端发送一个待条件的GET请求并且该请求以经被允许，而文档内容未被改变，则返回304,
    该响应不包含包体（即可直接使用缓存）。
403 Forbidden：服务器收到请求，但是拒绝提供服务。
404 Nott Found：请求资源不存在，举个例子：输入了错误的URL。
```
http 协议头相关
```
http数据由请求行，首部字段，空行，报文主体四个部分组成
首部字段分为：通用首部字段，请求首部字段，响应首部字段，实体首部字段
```
https与http的区别？如何实现加密传输？
```
https就是在http与传输层之间加上了一个SSL
对称加密与非对称加密
```
浏览器中输入一个URL发生什么，用到哪些协议？
```
    浏览器中输入URL，首先浏览器要将URL解析为IP地址，解析域名就要用到DNS协议，首先主机会查询DNS的缓存，如果没有就给本地DNS发送查询请求。
    DNS查询分为两种方式，一种是递归查询，一种是迭代查询。如果是迭代查询，本地的DNS服务器，向根域名服务器发送查询请求，
    根域名服务器告知该域名的一级域名服务器，然后本地服务器给该一级域名服务器发送查询请求，然后依次类推直到查询到该域名的IP地址。
    DNS服务器是基于UDP的，因此会用到UDP协议。得到IP地址后，浏览器就要与服务器建立一个http连接。因此要用到http协议，http协议报文格式上面已经提到。
    http生成一个get请求报文，将该报文传给TCP层处理。如果采用https还会先对http数据进行加密。TCP层如果有需要先将HTTP数据包分片，
    分片依据路径MTU和MSS。TCP的数据包然后会发送给IP层，用到IP协议。IP层通过路由选路，一跳一跳发送到目的地址。
    当然在一个网段内的寻址是通过以太网协议实现(也可以是其他物理层协议，比如PPP，SLIP)，以太网协议需要直到目的IP地址的物理地址，有需要ARP协议。
```

### 其他

### IO模型

五种IO模型
```
阻塞IO:
    最传统的一种IO模型，即在读写数据过程中会发生阻塞现象。
　　当用户线程发出IO请求之后，内核会去查看数据是否就绪，如果没有就绪就会等待数据就绪，而用户线程就会处于阻塞状态，
    用户线程交出CPU。当数据就绪之后，内核会将数据拷贝到用户线程，并返回结果给用户线程，用户线程才解除block状态

非阻塞IO:
　　当用户线程发起一个read操作后，并不需要等待，而是马上就得到了一个结果。
    如果结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦内核中的数据准备好了，
    并且又再次收到了用户线程的请求，那么它马上就将数据拷贝到了用户线程，然后返回。
　　所以事实上，在非阻塞IO模型中，用户线程需要不断地询问内核数据是否就绪，也就说非阻塞IO不会交出CPU，而会一直占用CPU。
IO复用:
　　多路复用IO模型是目前使用得比较多的模型。Java NIO实际上就是多路复用IO。
　　在多路复用IO模型中，会有一个线程不断去轮询多个socket的状态，只有当socket真正有读写事件时，才真正调用实际的IO读写操作。
    因为在多路复用IO模型中，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，
    并且只有在真正有socket读写事件进行时，才会使用IO资源，所以它大大减少了资源占用。
    另外多路复用IO为何比非阻塞IO模型的效率高是因为在非阻塞IO中，不断地询问socket状态时通过用户线程去进行的，
    而在多路复用IO中，轮询每个socket状态是内核在进行的，这个效率要比用户线程要高的多。
　　不过要注意的是，多路复用IO模型是通过轮询的方式来检测是否有事件到达，并且对到达的事件逐一进行响应。
    因此对于多路复用IO模型来说，一旦事件响应体很大，那么就会导致后续的事件迟迟得不到处理，并且会影响新的事件轮询。
信号驱动式IO:
    在信号驱动IO模型中，当用户线程发起一个IO请求操作，会给对应的socket注册一个信号函数，然后用户线程会继续执行，
    当内核数据就绪时会发送一个信号给用户线程，用户线程接收到信号之后，便在信号函数中调用IO读写操作来进行实际的IO请求操作。
    这个一般用于UDP中，对TCP套接口几乎是没用的，原因是该信号产生得过于频繁，并且该信号的出现并没有告诉我们发生了什么事情
异步IO:
    异步IO模型才是最理想的IO模型，在异步IO模型中，当用户线程发起read操作之后，立刻就可以开始去做其它的事。
    而另一方面，从内核的角度，当它受到一个asynchronous read之后，它会立刻返回，说明read请求已经成功发起了，
    因此不会对用户线程产生任何block。然后，内核会等待数据准备完成，然后将数据拷贝到用户线程，当这一切都完成之后，
    内核会给用户线程发送一个信号，告诉它read操作完成了。也就说用户线程完全不需要关心实际的整个IO操作是如何进行的，
    只需要先发起一个请求，当接收内核返回的成功信号时表示IO操作已经完成，可以直接去使用数据了。
　　也就说在异步IO模型中，IO操作的两个阶段都不会阻塞用户线程，这两个阶段都是由内核自动完成，
    然后发送一个信号告知用户线程操作已完成。用户线程中不需要再次调用IO函数进行具体的读写。
    这点是和信号驱动模型有所不同的，在信号驱动模型中，当用户线程接收到信号表示数据已经就绪，
    然后需要用户线程调用IO函数进行实际的读写操作；而在异步IO模型中，收到信号表示IO操作已经完成，
    不需要再在用户线程中调用iO函数进行实际的读写操作。
```
用户态内核态
```
内核态:
    操作系统内核对于硬件有不受限制的使用权限，并且可以执行任何 CPU 指令以及访问任意的内存地址。
用户态:
    进程没有能力直接操作硬件，也没有能力访问任意的内存地址空间。
    用户态进程只能通过操作系统内核提供的系统调用受限地使用硬件资源。
    并且用户态进程不能执行一些 CPU 特权指令。

系统调用属于内核空间代码，而用户态进程运行在用户态，所以执行系统调用时，程序的执行流程要从用户态切换到内核态。早期 Linux 通过中断的方式实现系统调用，中断号为 0x80。从用户态切换到内核态的代价是，首先要进行现场保护，即保存当前用户态的状态，以便执行完系统调用后恢复；其次是要从用户堆栈切换到内核堆栈；接着执行 0x80 号中断对应的中断处理程序，再通过中断处理程序找到相应的系统调用代码。系统调用执行完毕后又要切换堆栈、恢复现场。

从用户态到内核态切换可以通过三种方式：
系统调用：
        这个上面已经讲解过了，在我公众号之前的文章也有讲解过。
        其实系统调用本身就是中断，但是软件中断，跟硬中断不同。
异常：
    如果当前进程运行在用户态，如果这个时候发生了异常事件，就会触发切换。例如：缺页异常。
外设中断：
    当外设完成用户的请求时，会向CPU发送中断信号。
```

select，poll，epoll的区别
```
1、支持一个进程所能打开的最大连接数
select: 单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小
        （在32位的机器上，大小就是32*32，同理64位机器上FD_SETSIZE为32*64），当然我们可以对进行修改，
        然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试。
poll:   poll本质上和select没有区别，但是它没有最大连接数的限制，原因是它是基于链表来存储的
epoll:  虽然连接数有上限，但是很大，1G内存的机器上可以打开10万左右的连接，2G内存的机器可以打开20万左右的连接
2、FD剧增后带来的IO效率问题
select: 因为每次调用时都会对连接进行线性遍历，所以随着FD的增加会造成遍历速度慢的“线性下降性能问题”。

poll: 同上

epoll:  因为epoll内核中实现是根据每个fd上的callback函数来实现的，只有活跃的socket才会主动调用callback，
        所以在活跃socket较少的情况下，使用epoll没有前面两者的线性下降的性能问题，但是所有socket都很活跃的情况下，
        可能会有性能问题。
3、 消息传递方式
select: 内核需要将消息传递到用户空间，都需要内核拷贝动作
poll: 同上
epoll: epoll通过内核和用户空间共享一块内存来实现的。

总结:
    综上，在选择select，poll，epoll时要根据具体的使用场合以及这三种方式的自身特点。
1、表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，
    毕竟epoll的通知机制需要很多函数回调。
2、select低效是因为每次它都需要轮询。但低效也是相对的，视情况而定，也可通过良好的设计改善
关于这三种IO多路复用的用法，前面三篇总结写的很清楚，并用服务器回射echo程序进行了测试。连接如下所示：
select：
    是最初解决IO阻塞问题的方法。用结构体fd_set来告诉内核监听多个文件描述符，该结构体被称为描述符集。
    由数组来维持哪些描述符被置位了。
    对结构体的操作封装在三个宏定义中。通过轮寻来查找是否有描述符要被处理，如果没有返回
    存在的问题：
        内置数组的形式使得select的最大文件数受限与FD_SIZE；
        每次调用select前都要重新初始化描述符集，将fd从用户态拷贝到内核态，每次调用select后，都需要将fd从内核态拷贝到用户态；
        轮寻排查当文件描述符个数很多时，效率很低；
poll：
    通过一个可变长度的数组解决了select文件描述符受限的问题。
    数组中元素是结构体，该结构体保存描述符的信息，每增加一个文件描述符就向数组中加入一个结构体，结构体只需要拷贝一次到内核态。
    poll解决了select重复初始化的问题。轮寻排查的问题未解决。

epoll：
    轮寻排查所有文件描述符的效率不高，使服务器并发能力受限。
    因此，epoll采用只返回状态发生变化的文件描述符，便解决了轮寻的瓶颈。
```
为什么使用IO多路复用，最主要的原因是什么？
```
I/O multiplexing 这里面的 multiplexing 指的其实是在单个线程通过记录跟踪每一个Sock(I/O流)的状态
(对应空管塔里面的Fight progress strip槽)来同时管理多个I/O流. 发明它的原因，是尽量多的提高服务器的吞吐能力
```
epoll有两种触发模式？这两种触发模式有什么区别？编程的时候有什么区别？
```
LT模式下，只要一个句柄上的事件一次没有处理完，会在以后调用epoll_wait时次次返回这个句柄，
而ET模式仅在第一次返回.当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，
会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait干了件事，就是检查这些socket，
如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。

所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。
而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。
```
上一题中编程的时候有什么区别，是在边缘触发的时候要把套接字中的数据读干净，那么当有多个套接字时，
在读的套接字一直不停的有数据到达，如何保证其他套接字不被饿死
```
使用超时时间吧，while循环超过了指定的时间间隔，就打断循环体。
```
epoll为什么这么快
```
这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，
在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，
当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，
等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。
```
nagle算法；
keepalive选项；
Linger选项；
对于某一端出现大量CLOSE_WAIT 或者 TIME_WAIT如何解决；
```
1:服务器保持了大量TIME_WAIT状态
    这种情况比较常见，一些爬虫服务器或者WEB服务器（如果网管在安装的时候没有做内核参数优化的话）上经常会遇到这个问题，
    这个问题是怎么产生的呢？
    从上面的示意图可以看得出来，TIME_WAIT是主动关闭连接的一方保持的状态，对于爬虫服务器来说他本身就是“客户端”，
    在完成一个爬取任务之后，他就会发起主动关闭连接，从而进入TIME_WAIT的状态，
    然后在保持这个状态2MSL（max segment lifetime）时间之后，彻底关闭回收资源。
    为什么要这么做？
    明明就已经主动关闭连接了为啥还要保持资源一段时间呢？
这个是TCP/IP的设计者规定的，主要出于以下两个方面的考虑：
    1.防止上一次连接中的包，迷路后重新出现，影响新连接（经过2MSL，上一次连接中所有的重复包都会消失）
    2.可靠的关闭TCP连接。在主动关闭方发送的最后一个 ack(fin) ，有可能丢失，这时被动方会重新发fin, 
        如果这时主动方处于 CLOSED 状态 ，就会响应 rst 而不是 ack。所以主动方要处于 TIME_WAIT 状态，而不能是 CLOSED 。
        另外这么设计TIME_WAIT 会定时的回收资源，并不会占用很大资源的，除非短时间内接受大量请求或者受到攻击。

2.服务器保持了大量CLOSE_WAIT状态
    TIME_WAIT状态可以通过优化服务器参数得到解决，因为发生TIME_WAIT的情况是服务器自己可控的，
    要么就是对方连接的异常，要么就是自己没有迅速回收资源，总之不是由于自己程序错误导致的。
    但是CLOSE_WAIT就不一样了，从上面的图可以看出来，如果一直保持在CLOSE_WAIT状态，
    那么只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出ack信号。
    换句话说，就是在对方连接关闭之后，程序里没有检测到，或者程序压根就忘记了这个时候需要关闭连接，
    于是这个资源就一直被程序占着。个人觉得这种情况，通过服务器内核参数也没办法解决，
    服务器对于程序抢占的资源没有主动回收的权利，除非终止程序运行。
```
通讯协议如何设计或如何解决数据包的粘包与分片问题；
```
固定包长的数据包:
    每个协议包的长度都是固定的。
    举个例子，例如我们可以规定每个协议包的大小是 64 个字节，每次收满 64 个字节，就取出来解析（如果不够，就先存起来）。
    这种通信协议的格式简单但灵活性差。如果包内容不足指定的字节数，剩余的空间需要填充特殊的信息，
    如 \0（如果不填充特殊内容，如何区分包里面的正常内容与填充信息呢？）；
    如果包内容超过指定字节数，又得分包分片，需要增加额外处理逻辑——在发送端进行分包分片，
    在接收端重新组装包片（分包和分片内容在接下来会详细介绍）。

以指定字符（串）为包的结束标志:
    字节流中遇到特殊的符号值时就认为到一个包的末尾了。
    例如，我们熟悉的 FTP协议，发邮件的 SMTP 协议，一个命令或者一段数据后面加上"\r\n"（即所谓的 CRLF）表示一个包的结束。
    对端收到后，每遇到一个”\r\n“就把之前的数据当做一个数据包。
    这种协议一般用于一些包含各种命令控制的应用中，其不足之处就是如果协议数据包内容部分需要使用包结束标志字符，
    就需要对这些字符做转码或者转义操作，以免被接收方错误地当成包结束标志而误解析。
包头 + 包体格式:
    取包头时，你应该拷贝一份数据包头大小的数据出来，而不是从缓冲区 pBuffer 中直接将数据取出来
    （即取出来的数据从 pBuffer 中移除，这是因为倘若接下来根据包头中的字段得到包体大小时，如果剩余数据不够一个包体大小，
    你又得把这个包头数据放回缓冲区。为了避免这种不必要的操作，只有缓冲区数据大小够整个包的大小
    （代码中：header.bodysize + sizeof(msg)）你才需要把整个包大小的数据从缓冲区移除，
    这也是这里的 pBuffer->peek() 方法 peek 单词的含义（中文可以翻译成“瞟一眼”或者“偷窥”）。

    通过包头得到包体大小时，你一定要对 bodysize 的数值进行校验，
    我这里要求 bodysize 必须大于 0 且不大于 10 * 1024 * 1024（即 10 M）。当然，实际开发中，
    你可以根据你自己的需求要决定 bodysize 的上下限（包体大小是 0 字节的包在某些业务场景下是允许的）。
    记住，一定要判断这个上下限，因为假设这是一个非法的客户端发来的数据，其 bodysize 设置了一个比较大的数值，
    例如 1 * 1024 * 1024 * 1024（即 1 G），你的逻辑会让你一直缓存该客户端发来的数据，
    那么很快你的服务器内存将会被耗尽，操作系统在检测到你的进程占用内存达到一定阈值时会杀死你的进程，
    导致服务不能再正常对外服务。如果你检测了 bodysize 字段的是否满足你设置的上下限，
    对于非法的 bodysize，直接关闭这路连接即可。这也是服务的一种自我保护措施，避免因为非法数据包带来的损失。

    不知道你有没有注意到整个判断包头、包体以及处理包的逻辑放在一个 while 循环里面，这是必要的。
    如果没有这个 while 循环，当你一次性收到多个包时，你只会处理一个，下次接着处理就需要等到新一批数据来临时再次触发这个逻辑。
    这样造成的结果就是，对端给你发送了多个请求，你最多只能应答一个，后面的应答得等到对端再次给你发送数据时。
    这就是对粘包逻辑的正确处理
```
心跳机制如何设计；（可能不会直接问问题本身，如问如何检查死链）
断线重连机制如何设计；
```
客户端的实现：
    客户端有一个队列，记录了已发送的request包，收到服务器的response后，再删除对应的request。
    如果超时没有收到response，可以认为发生了断线，重新发送缓存的request，将pkgid设置为负标记为重传包。
    重试有次数限制，如果超过次数仍然收不到回复，就提示断线，退出到登录界面。
服务器的实现：
    服务器开一个缓存池，记录近期一定数量的response包和notify推送包。
    当收到了重传包时，通过玩家id+pkgid，从缓存池里索引近期缓存的response包，缓存命中直接下发记录，
    没有命中则表示重传失败，服务器做踢线处理（踢线之后客户端会继续走登录流程）。
```
收发数据包正确的方式，收发缓冲区如何设计；
```
为什么要自定义缓冲区:
    假设应用程序需要发送40kB数据,但是操作系统的TCP发送缓冲区只有25kB剩余空间,那么剩下的15kB数据怎么办?
    如果等待OS缓冲区可用,会阻塞当前线程,因为不知道对方什么时候收到并读取数据。
    因此网络库应该把这15kB数据缓存起来,放到这个TCP连接的应用层发送缓冲区中,等socket变得可写的时候立刻发送数据,
    这样“发送”操作不会阻塞。如果应用程序随后又要发送50kB数据,而此时发送缓冲区中尚有未发送的数据(若干kB),
    那么网络库应该将这50kB数据追加到发送缓冲区的末尾,而不能立刻尝试write(),因为这样有可能打乱数据的顺序。
    另外的话，假如一次读到的数据不够一个完整的数据包,那么这些已经读到的数据是不是应该先暂存在某个地方,
    等剩余的数据收到之后再一并处理。
1.2 缓冲区设计的原则
    一方面希望减少系统调用,一次读的数据越多越划算,那么应该准备一个大的缓冲区；
    另一方面,希望减少内存占用。如果有10000个并发连接,每个连接一建立就分配各50kB的读写缓冲区(s)的话,将占用1GB内存,
    而大多数时候这些缓冲区的使用率很低，可以用readv(2)结合栈上空间解决了这个问题；
1.3 建立缓冲区的方式
    1.3.1 每次都重新申请缓冲区
    每次接收到数据的时候开辟一个缓冲区，然后将接收到的数据填入缓冲区，把缓冲区和IP信息付给任务，压入到任务队列，
    等任务线程处理; 发送亦然;(小数据可以用栈拷贝的形式)
    好处：是接收线程可以一直接收，任务线程一直处理,除了任务锁没有其他交互；
    缺点: 每次都重新申请空间,malloc(或new)消耗量大(可以使用内存池优化)；
1.3.2 预先申请缓冲区
    预先申请一块大的缓冲区(每个连接各申请一个接收缓冲区和发送缓冲区)，
    接收线程有新数据到来的时候从缓冲区中数据结尾获得可用空间插入数据,
    把连接信息和整个缓冲区压入任务队列,任务线程处理一个任务的数据，就清空缓冲区该段的数据，
    然后将缓冲区中后面的数据前移(所以每次都是处理的第一个数据区的数据)
    好处:减少了malloc
    缺点:在数据插入和使用的时候都使用的锁，而且有严重的拷贝复制情况，
    (如果想任务处理和数据接收不互相锁，必须使用多的一份儿数据拷贝,情况就更糟)
```

优雅关闭；
定时器如何设计；
```
必要的数据结构
优先级任务队列：队列中存储任务，每个任务会添加时间戳，最近的时间戳的任务会先出队。
锁和条件变量：当有任务需要执行时，用于通知正在等待的线程从任务队列中取出任务执行。
线程池：各个任务会放在线程池中执行

一个常用的做法是新起一个线程，专门管理定时器，定时来源使用rtc、select等比较精确的来源，定时器超时后向主要的work线程发消息即可，或者使用timefd接口
```
epoll 的实现原理。
